{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Рекурентные сети для обработки последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В данной теме надо будет:\n",
    "\n",
    "* Попробуем обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "* Сделать выводы"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymystem3 import Mystem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vlad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vlad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vlad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 2000\n",
    "MAX_LEN = 20\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# Training\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 512\n",
    "PRINT_BATCH_N = 100\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "SW = set(get_stop_words('english'))\n",
    "PUNCTS = set(punctuation)\n",
    "\n",
    "TYPE_LEMM = 'WordNetLemmatizer'\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Для эксперемента используем несколько библиотек лемматизации\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "mystem_lemmatizer = Mystem()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/GRU_LSTM/train.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Функции, которые нам понадобятся"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def lemmatizer_text(txt, TYPE_LEMM):\n",
    "    result_txt = []\n",
    "    for word in txt.split():\n",
    "        if word in SW:\n",
    "            continue\n",
    "        if TYPE_LEMM == 'WordNetLemmatizer':\n",
    "            result_txt.append(wn_lemmatizer.lemmatize(word))\n",
    "        elif TYPE_LEMM == 'Mystem':\n",
    "            m_l = mystem_lemmatizer.lemmatize(word)\n",
    "            if (len(m_l) > 0):\n",
    "                result_txt.append(m_l[0])\n",
    "    return result_txt\n",
    "\n",
    "# Функция для предобработки текста\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in PUNCTS)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"не\\s\", \"не\", txt)\n",
    "    txt = re.sub(r\"@[\\w]*\", \"\", txt)\n",
    "    txt = re.sub(r'[^\\w\\s]', \" \", txt)\n",
    "    txt = re.sub(r\"[^a-zA-Z0-9]\",\" \", txt)\n",
    "    txt = re.sub(r\"[^a-zA-Z0-9]\",\" \", txt)\n",
    "    txt = lemmatizer_text(txt, TYPE_LEMM)\n",
    "    return \" \".join(txt)\n",
    "\n",
    "# Функция для сборки последовательности\n",
    "def text_to_sequence(text, maxlen, vocabulary):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Опишем нейронную сеть (с учетом выбора типа модели) и Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "#\n",
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "#\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=MAX_WORDS, type_model='rnn', embedding_dim=128, hidden_dim=128, use_last=True) -> None:\n",
    "        super().__init__()\n",
    "        self.use_last = use_last\n",
    "        self.type_model = type_model\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if type_model == 'rnn':\n",
    "            self.model = nn.RNN(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        elif type_model == 'lstm':\n",
    "            self.model = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        elif type_model == 'gru':\n",
    "            self.model = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embeddings(x)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        model_out, ht = self.model(output)\n",
    "        if self.use_last:\n",
    "            last_tensor = model_out[:,-1,:]\n",
    "        else:\n",
    "            # use mean\n",
    "            last_tensor = torch.mean(model_out[:,:], dim=1)\n",
    "        output = self.linear(last_tensor)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Предобаботаем данныые"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess for lemma: WordNetLemmatizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:01<00:00, 21110.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break token for lemma: WordNetLemmatizer\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.630. Acc: 0.938. Test loss: 0.646. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.361. Acc: 0.941. Test loss: 0.559. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.292. Acc: 0.920. Test loss: 0.615. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.248. Acc: 0.934. Test loss: 0.675. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.259. Acc: 0.928. Test loss: 0.702. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.254. Acc: 0.930. Test loss: 1.831. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.218. Acc: 0.939. Test loss: 0.766. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.256. Acc: 0.930. Test loss: 0.780. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.253. Acc: 0.926. Test loss: 0.663. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.216. Acc: 0.938. Test loss: 0.844. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.683. Acc: 0.932. Test loss: 0.686. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.590. Acc: 0.932. Test loss: 0.636. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.525. Acc: 0.920. Test loss: 0.603. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.451. Acc: 0.953. Test loss: 0.582. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.436. Acc: 0.914. Test loss: 0.570. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.369. Acc: 0.951. Test loss: 0.611. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.231. Acc: 0.932. Test loss: 0.712. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.169. Acc: 0.953. Test loss: 0.758. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.266. Acc: 0.918. Test loss: 0.792. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.192. Acc: 0.953. Test loss: 0.736. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.721. Acc: 0.066. Test loss: 0.704. Test acc: 0.071.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.578. Acc: 0.918. Test loss: 0.625. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.479. Acc: 0.920. Test loss: 0.582. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.395. Acc: 0.934. Test loss: 0.563. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.338. Acc: 0.938. Test loss: 0.561. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.293. Acc: 0.943. Test loss: 1.095. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.203. Acc: 0.947. Test loss: 0.744. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.245. Acc: 0.928. Test loss: 0.648. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.281. Acc: 0.914. Test loss: 0.691. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.284. Acc: 0.908. Test loss: 0.698. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.689. Acc: 0.691. Test loss: 0.689. Test acc: 0.876.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.451. Acc: 0.928. Test loss: 0.576. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.325. Acc: 0.930. Test loss: 0.566. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.295. Acc: 0.920. Test loss: 0.606. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.296. Acc: 0.914. Test loss: 0.646. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.253. Acc: 0.932. Test loss: 1.653. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.254. Acc: 0.930. Test loss: 0.746. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.229. Acc: 0.941. Test loss: 0.737. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.260. Acc: 0.922. Test loss: 0.729. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.213. Acc: 0.941. Test loss: 0.732. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.711. Acc: 0.055. Test loss: 0.701. Test acc: 0.071.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.623. Acc: 0.926. Test loss: 0.652. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.548. Acc: 0.943. Test loss: 0.618. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.501. Acc: 0.928. Test loss: 0.594. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.469. Acc: 0.912. Test loss: 0.579. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.419. Acc: 0.934. Test loss: 0.560. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.237. Acc: 0.932. Test loss: 0.674. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.191. Acc: 0.951. Test loss: 0.798. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.209. Acc: 0.941. Test loss: 0.767. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.258. Acc: 0.918. Test loss: 0.731. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.679. Acc: 0.926. Test loss: 0.685. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.569. Acc: 0.918. Test loss: 0.625. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.484. Acc: 0.928. Test loss: 0.590. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.424. Acc: 0.926. Test loss: 0.571. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.383. Acc: 0.922. Test loss: 0.564. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: SGD, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.317. Acc: 0.949. Test loss: 0.738. Test acc: 0.929.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.217. Acc: 0.939. Test loss: 0.645. Test acc: 0.929.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.266. Acc: 0.920. Test loss: 0.660. Test acc: 0.929.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.227. Acc: 0.941. Test loss: 0.790. Test acc: 0.929.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.226. Acc: 0.936. Test loss: 0.771. Test acc: 0.929.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10, MAX_LEN: 5, optim: Adam, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.723. Acc: 0.061. Test loss: 0.715. Test acc: 0.079.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.414. Acc: 0.941. Test loss: 0.355. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.261. Acc: 0.934. Test loss: 0.125. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.254. Acc: 0.930. Test loss: 0.083. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.310. Acc: 0.908. Test loss: 0.076. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.230. Acc: 0.939. Test loss: 0.002. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.457. Acc: 0.729. Test loss: 0.260. Test acc: 0.730.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.358. Acc: 0.922. Test loss: 0.170. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.295. Acc: 0.938. Test loss: 0.163. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.270. Acc: 0.926. Test loss: 0.129. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.715. Acc: 0.078. Test loss: 0.715. Test acc: 0.069.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.611. Acc: 0.939. Test loss: 0.597. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.546. Acc: 0.908. Test loss: 0.506. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.483. Acc: 0.922. Test loss: 0.434. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.434. Acc: 0.928. Test loss: 0.376. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.410. Acc: 0.914. Test loss: 0.056. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.238. Acc: 0.932. Test loss: 0.117. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.262. Acc: 0.920. Test loss: 0.111. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.206. Acc: 0.941. Test loss: 0.150. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.185. Acc: 0.947. Test loss: 0.161. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.701. Acc: 0.068. Test loss: 0.697. Test acc: 0.098.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.550. Acc: 0.936. Test loss: 0.524. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.435. Acc: 0.947. Test loss: 0.395. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.373. Acc: 0.930. Test loss: 0.297. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.315. Acc: 0.936. Test loss: 0.225. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.296. Acc: 0.928. Test loss: 0.001. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.222. Acc: 0.938. Test loss: 0.089. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.198. Acc: 0.939. Test loss: 0.183. Test acc: 0.932.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.224. Acc: 0.930. Test loss: 0.143. Test acc: 0.932.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.172. Acc: 0.945. Test loss: 0.184. Test acc: 0.933.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.688. Acc: 0.830. Test loss: 0.677. Test acc: 0.921.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.427. Acc: 0.932. Test loss: 0.363. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.335. Acc: 0.904. Test loss: 0.174. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.257. Acc: 0.932. Test loss: 0.105. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.213. Acc: 0.947. Test loss: 0.085. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.255. Acc: 0.930. Test loss: 0.001. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.252. Acc: 0.930. Test loss: 0.075. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.292. Acc: 0.910. Test loss: 0.121. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.229. Acc: 0.930. Test loss: 0.118. Test acc: 0.933.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.175. Acc: 0.947. Test loss: 0.104. Test acc: 0.932.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.720. Acc: 0.068. Test loss: 0.721. Test acc: 0.069.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.621. Acc: 0.932. Test loss: 0.607. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.540. Acc: 0.945. Test loss: 0.518. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.473. Acc: 0.955. Test loss: 0.446. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.432. Acc: 0.943. Test loss: 0.389. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.411. Acc: 0.924. Test loss: 0.137. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.242. Acc: 0.930. Test loss: 0.122. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.197. Acc: 0.938. Test loss: 0.104. Test acc: 0.932.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.210. Acc: 0.930. Test loss: 0.219. Test acc: 0.932.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.204. Acc: 0.934. Test loss: 0.264. Test acc: 0.934.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.641. Acc: 0.930. Test loss: 0.628. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.517. Acc: 0.938. Test loss: 0.485. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.428. Acc: 0.939. Test loss: 0.378. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.380. Acc: 0.924. Test loss: 0.298. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.348. Acc: 0.918. Test loss: 0.239. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: SGD, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.286. Acc: 0.941. Test loss: 0.007. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.250. Acc: 0.914. Test loss: 0.156. Test acc: 0.932.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.275. Acc: 0.910. Test loss: 0.323. Test acc: 0.933.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.171. Acc: 0.949. Test loss: 0.176. Test acc: 0.933.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.204. Acc: 0.930. Test loss: 0.236. Test acc: 0.934.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 100, MAX_LEN: 10, optim: Adam, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.712. Acc: 0.057. Test loss: 0.704. Test acc: 0.072.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.428. Acc: 0.916. Test loss: 0.350. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.264. Acc: 0.934. Test loss: 0.133. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.208. Acc: 0.949. Test loss: 0.085. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.240. Acc: 0.936. Test loss: 0.076. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.295. Acc: 0.914. Test loss: 0.496. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.263. Acc: 0.928. Test loss: 0.063. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.214. Acc: 0.945. Test loss: 0.095. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.265. Acc: 0.928. Test loss: 0.078. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.306. Acc: 0.914. Test loss: 0.156. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.667. Acc: 0.920. Test loss: 0.659. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.580. Acc: 0.926. Test loss: 0.557. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.510. Acc: 0.934. Test loss: 0.476. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.455. Acc: 0.938. Test loss: 0.411. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.415. Acc: 0.934. Test loss: 0.359. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.382. Acc: 0.934. Test loss: 0.015. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.254. Acc: 0.930. Test loss: 0.071. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.297. Acc: 0.914. Test loss: 0.072. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.255. Acc: 0.930. Test loss: 0.066. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.279. Acc: 0.920. Test loss: 0.082. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.699. Acc: 0.076. Test loss: 0.695. Test acc: 0.072.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.550. Acc: 0.934. Test loss: 0.522. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.448. Acc: 0.936. Test loss: 0.400. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.376. Acc: 0.936. Test loss: 0.309. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.340. Acc: 0.924. Test loss: 0.240. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.288. Acc: 0.938. Test loss: 0.002. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.216. Acc: 0.945. Test loss: 0.080. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.245. Acc: 0.934. Test loss: 0.053. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.244. Acc: 0.934. Test loss: 0.066. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.218. Acc: 0.943. Test loss: 0.067. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.749. Acc: 0.074. Test loss: 0.746. Test acc: 0.072.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.414. Acc: 0.924. Test loss: 0.340. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.251. Acc: 0.939. Test loss: 0.130. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.288. Acc: 0.916. Test loss: 0.087. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.205. Acc: 0.949. Test loss: 0.076. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.255. Acc: 0.930. Test loss: 0.004. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.230. Acc: 0.941. Test loss: 0.081. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.240. Acc: 0.934. Test loss: 0.058. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.251. Acc: 0.932. Test loss: 0.081. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.263. Acc: 0.920. Test loss: 0.112. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.650. Acc: 0.943. Test loss: 0.642. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.567. Acc: 0.920. Test loss: 0.538. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.485. Acc: 0.953. Test loss: 0.457. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.437. Acc: 0.941. Test loss: 0.393. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.400. Acc: 0.936. Test loss: 0.340. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.377. Acc: 0.926. Test loss: 0.006. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.189. Acc: 0.943. Test loss: 0.037. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.137. Acc: 0.953. Test loss: 0.033. Test acc: 0.930.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.198. Acc: 0.947. Test loss: 0.015. Test acc: 0.942.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.132. Acc: 0.949. Test loss: 0.020. Test acc: 0.939.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.686. Acc: 0.936. Test loss: 0.679. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.535. Acc: 0.941. Test loss: 0.506. Test acc: 0.928.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.435. Acc: 0.936. Test loss: 0.384. Test acc: 0.928.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.370. Acc: 0.930. Test loss: 0.292. Test acc: 0.928.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.317. Acc: 0.934. Test loss: 0.224. Test acc: 0.928.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: SGD, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.293. Acc: 0.930. Test loss: 0.002. Test acc: 0.928.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.175. Acc: 0.936. Test loss: 0.006. Test acc: 0.944.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.112. Acc: 0.961. Test loss: 0.004. Test acc: 0.949.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.093. Acc: 0.973. Test loss: 0.001. Test acc: 0.949.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.098. Acc: 0.963. Test loss: 0.001. Test acc: 0.949.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 1000, MAX_LEN: 100, optim: Adam, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.691. Acc: 0.945. Test loss: 0.678. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.321. Acc: 0.938. Test loss: 0.231. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.249. Acc: 0.934. Test loss: 0.096. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.250. Acc: 0.932. Test loss: 0.078. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.219. Acc: 0.943. Test loss: 0.073. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.214. Acc: 0.945. Test loss: 0.001. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.257. Acc: 0.930. Test loss: 0.115. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.210. Acc: 0.951. Test loss: 0.091. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.244. Acc: 0.934. Test loss: 0.077. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.261. Acc: 0.930. Test loss: 0.084. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: rnn, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.657. Acc: 0.938. Test loss: 0.649. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.573. Acc: 0.922. Test loss: 0.547. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.510. Acc: 0.918. Test loss: 0.466. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.449. Acc: 0.934. Test loss: 0.400. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.405. Acc: 0.936. Test loss: 0.347. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.379. Acc: 0.928. Test loss: 0.010. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.259. Acc: 0.928. Test loss: 0.072. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.287. Acc: 0.918. Test loss: 0.065. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.289. Acc: 0.916. Test loss: 0.077. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.224. Acc: 0.941. Test loss: 0.075. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: lstm, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.659. Acc: 0.930. Test loss: 0.649. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.514. Acc: 0.936. Test loss: 0.479. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.421. Acc: 0.926. Test loss: 0.356. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.360. Acc: 0.922. Test loss: 0.266. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.294. Acc: 0.939. Test loss: 0.203. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.280. Acc: 0.932. Test loss: 0.001. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.270. Acc: 0.924. Test loss: 0.073. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.306. Acc: 0.912. Test loss: 0.062. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.276. Acc: 0.922. Test loss: 0.063. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.275. Acc: 0.922. Test loss: 0.090. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: gru, use_last: True  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.676. Acc: 0.924. Test loss: 0.665. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.379. Acc: 0.934. Test loss: 0.307. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.267. Acc: 0.930. Test loss: 0.124. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.279. Acc: 0.920. Test loss: 0.087. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.268. Acc: 0.924. Test loss: 0.077. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.219. Acc: 0.943. Test loss: 0.001. Test acc: 0.926.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.253. Acc: 0.930. Test loss: 0.076. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.335. Acc: 0.922. Test loss: 0.151. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.261. Acc: 0.910. Test loss: 0.075. Test acc: 0.912.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.287. Acc: 0.914. Test loss: 0.084. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: rnn, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.723. Acc: 0.082. Test loss: 0.725. Test acc: 0.069.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.616. Acc: 0.951. Test loss: 0.605. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.536. Acc: 0.943. Test loss: 0.511. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.472. Acc: 0.943. Test loss: 0.437. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.430. Acc: 0.934. Test loss: 0.377. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.384. Acc: 0.941. Test loss: 0.026. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.191. Acc: 0.939. Test loss: 0.045. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.174. Acc: 0.934. Test loss: 0.031. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.162. Acc: 0.947. Test loss: 0.032. Test acc: 0.944.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.070. Acc: 0.975. Test loss: 0.005. Test acc: 0.955.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: lstm, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.670. Acc: 0.938. Test loss: 0.663. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.524. Acc: 0.926. Test loss: 0.486. Test acc: 0.931.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.425. Acc: 0.922. Test loss: 0.357. Test acc: 0.931.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.344. Acc: 0.934. Test loss: 0.263. Test acc: 0.931.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.291. Acc: 0.939. Test loss: 0.198. Test acc: 0.931.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: SGD, type_model: gru, use_last: False  finished!\n",
      "Epoch [1/5]. Step [1/42]. Loss: 0.274. Acc: 0.934. Test loss: 0.001. Test acc: 0.931.\n",
      "Epoch [2/5]. Step [1/42]. Loss: 0.226. Acc: 0.918. Test loss: 0.148. Test acc: 0.932.\n",
      "Epoch [3/5]. Step [1/42]. Loss: 0.136. Acc: 0.951. Test loss: 0.029. Test acc: 0.949.\n",
      "Epoch [4/5]. Step [1/42]. Loss: 0.087. Acc: 0.969. Test loss: 0.030. Test acc: 0.957.\n",
      "Epoch [5/5]. Step [1/42]. Loss: 0.102. Acc: 0.971. Test loss: 0.019. Test acc: 0.957.\n",
      "Training for lem: WordNetLemmatizer, MAX_WORDS: 10000, MAX_LEN: 100, optim: Adam, type_model: gru, use_last: False  finished!\n"
     ]
    }
   ],
   "source": [
    "for TYPE_LEMM in ['WordNetLemmatizer']:\n",
    "    print(f\"Preprocess for lemma: {TYPE_LEMM}\")\n",
    "    # предобрабатываем текст\n",
    "    df_train['pre_proc_tweet_' + TYPE_LEMM] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "\n",
    "    print(f\"Break token for lemma: {TYPE_LEMM}\")\n",
    "    # разбиваем на токены\n",
    "    tokens = word_tokenize(\" \".join(df_train['pre_proc_tweet_' + TYPE_LEMM]))\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for (MAX_WORDS, MAX_LEN) in [(10, 5), (100, 10), (1000, 100), (10000, 100)]:\n",
    "        # создаем словарь\n",
    "        dist = FreqDist(tokens_filtered)\n",
    "        tokens_filtered_top = [pair[0] for pair in dist.most_common(MAX_WORDS-1)]  # вычитание 1 для padding\n",
    "        vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n",
    "\n",
    "        x_train = np.asarray([text_to_sequence(text, MAX_LEN, vocabulary) for text in df_train['pre_proc_tweet_' + TYPE_LEMM]])\n",
    "\n",
    "        # Загружаем данные в dataset\n",
    "        train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
    "        # Разбиваем наши данные на тренеровочные и тестовые с параметрамми для трейна = 67% и теста = 33%\n",
    "        train_dataset, test_dataset = train_test_split(train_dataset, train_size=0.67, test_size=0.33)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        for use_last in [True, False]:\n",
    "            for type_model in ['rnn', 'lstm', 'gru']:\n",
    "\n",
    "                # создаем нашу модель\n",
    "                model = Net(vocab_size=MAX_WORDS, type_model=type_model, embedding_dim=(MAX_LEN-1), use_last=use_last)\n",
    "\n",
    "                for optim in ['SGD', 'Adam']:\n",
    "                    # Оптимизатор и функция потерь\n",
    "                    if optim == 'SGD':\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "                    else:\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "                    criterion = nn.BCELoss()\n",
    "\n",
    "                    model = model.to(DEVICE)\n",
    "                    th = 0.5\n",
    "\n",
    "                    train_loss_history = []\n",
    "                    test_loss_history = []\n",
    "\n",
    "                    for epoch in range(EPOCHS):\n",
    "                        model.train()\n",
    "                        running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "                        for i, train_data in enumerate(train_loader, 0):\n",
    "                            inputs, labels = train_data[0].to(DEVICE), train_data[1].to(DEVICE)\n",
    "\n",
    "                            # обнуляем градиент\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(inputs)\n",
    "\n",
    "                            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            # подсчет ошибки на обучении\n",
    "                            loss = loss.item()\n",
    "                            running_items += len(labels)\n",
    "                            # подсчет метрики на обучении\n",
    "                            pred_labels = torch.squeeze((outputs > th).int())\n",
    "                            running_right += (labels == pred_labels).sum()\n",
    "\n",
    "                            # выводим статистику о процессе обучения\n",
    "                            if i % 150 == 0:    # печатаем каждые 150 batches\n",
    "                                model.eval()\n",
    "\n",
    "                                # f1_score_train = f1_score(labels, pred_labels, zero_division=1)\n",
    "                                # precision_score_train = precision_score(labels, pred_labels, zero_division=1)\n",
    "                                # recall_score_train = recall_score(labels, pred_labels, zero_division=1)\n",
    "\n",
    "                                print(f'Epoch [{epoch + 1}/{EPOCHS}]. ' +\n",
    "                                      f'Step [{i + 1}/{len(train_loader)}]. ' +\n",
    "                                      f'Loss: {loss:.3f}. ' +\n",
    "                                      f'Acc: {running_right / running_items:.3f}', end='. ')\n",
    "                                      # f'f1_score: {f1_score_train:.3f}. ' +\n",
    "                                      # f'precision_score: {precision_score_train:.3f}. ' +\n",
    "                                      # f'recall_score: {recall_score_train:.3f}.'\n",
    "\n",
    "                                running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "                                train_loss_history.append(loss)\n",
    "\n",
    "                                # выводим статистику на тестовых данных\n",
    "                                test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
    "                                for j, test_data in enumerate(test_loader):\n",
    "                                    test_labels = test_data[1]\n",
    "                                    test_outputs = model(test_data[0])\n",
    "\n",
    "                                    # подсчет ошибки на тесте\n",
    "                                    test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
    "                                    # подсчет метрики на тесте\n",
    "                                    test_running_total += len(test_data[1])\n",
    "                                    pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
    "                                    test_running_right += (test_labels == pred_test_labels).sum()\n",
    "\n",
    "                                    # f1_score_test = f1_score(test_labels, pred_test_labels, zero_division=1)\n",
    "                                    # precision_score_test = precision_score(test_labels, pred_test_labels, zero_division=1)\n",
    "                                    # recall_score_test = recall_score(test_labels, pred_test_labels, zero_division=1)\n",
    "\n",
    "                                test_loss_history.append(test_loss.item())\n",
    "                                print(f'Test loss: {test_loss:.3f}. '\n",
    "                                      f'Test acc: {test_running_right / test_running_total:.3f}.')\n",
    "                                      # f'Test f1_score_test: {f1_score_test}. '\n",
    "                                      # f'Test precision_score: {precision_score_test}. '\n",
    "                                      # f'Test recall_score_test: {recall_score_test}')\n",
    "\n",
    "                            model.train()\n",
    "\n",
    "                    print(f'Training for lem: {TYPE_LEMM}, MAX_WORDS: {MAX_WORDS}, MAX_LEN: {MAX_LEN}, optim: {optim}, type_model: {type_model}, use_last: {str(use_last)}  finished!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вывод"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Было проведено 48 тестрирований (результаты представлены на результате 5-ой эпохи), для лемматизации использовался - WordNetLemmatizer:\n",
    "\n",
    "С вариациями:\n",
    "\n",
    "1) Изменение количества максимальных слов: 10, 100, 1000, 100000 и максимальную длинну: 5, 10, 100, 100\n",
    "\n",
    "2) Изменение метода оптимизатора: SGD, Adam\n",
    "\n",
    "3) Изменение разновидности нейронных сетей: rnn, lstm, gru\n",
    "\n",
    "4) Использовать результат предыдущего вычисления: Tru, False\n",
    "\n",
    "**Результат**:\n",
    "\n",
    "Сразу оговорюсь, результат учитывался именно в последней эпохи.\n",
    "\n",
    "Наилучшим вариант был при варианте: количества максимальных слов (MAX_WORDS) - 10000, максимальной длинны (MAX_LEN) - 100, оптимизатор: Adam, тип нейронной сети: gru, использование предыдущего результата: False.\n",
    "\n",
    "Худшими вариантами были при вариантах: количества максимальных слов (MAX_WORDS) - 1000, максимальной длинны (MAX_LEN) - 100, оптимизатор: SGD и Adam,\n",
    "тип нейронной сети: rnn, lstm, gru, использование предыдущего результата: True, False.\n",
    "\n",
    "**Вывод**\n",
    "\n",
    "Похоже для обучение нейронной сети посредством rnn, lstm, gru лучше использовать побольшей количества слов и оптимизатор Adam.\n",
    "\n",
    "P.S. были попытки использовать леммитизатор от бибилиотеки: pymystem3, но его скорость сильно уступает WordNetLemmatizer, потому в последствии пришлось отказаться от него."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}