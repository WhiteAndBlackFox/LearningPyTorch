{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFsk/Q/ifTvkEKIAufdJiR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WhiteAndBlackFox/LearningPyTorch/blob/CNN/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN свертки"
      ],
      "metadata": {
        "id": "Ds37XlcHNSgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здача будет разбита на 3 пункта:\n",
        "* Обучить CNN (самописная) на CIFAR-100.\n",
        "* Обучить CNN на CIFAR-100 через дообучение ImageNet Resnet-50.\n",
        "* Обучить CNN на CIFAR-100 через дообучение ImageNet Resnet-50 с аугментацией данных.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bzh-eX6wNcXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подключаем библиотеки"
      ],
      "metadata": {
        "id": "dTbODyq_v3p0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tpIV6P3wNNr4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.datasets.utils import download_and_extract_archive, check_integrity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Глобальные переменные"
      ],
      "metadata": {
        "id": "NrUwBCDG5qWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "STATS = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "EPOCHES = 5\n",
        "\n",
        "PATH_DOWNLOAD = 'data/'"
      ],
      "metadata": {
        "id": "iBeCk85U5qqT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZMv6x3GA6zl",
        "outputId": "96a86526-7091-42de-e1ef-7739ab6e80fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Реализация"
      ],
      "metadata": {
        "id": "zRHKlqjM6Jlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучить CNN (самописная) на CIFAR-100"
      ],
      "metadata": {
        "id": "uZZpZsQh6Mwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Создадим Dataset"
      ],
      "metadata": {
        "id": "Qt55WYD_dm9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cifar100Dataset(Dataset):\n",
        "   \n",
        "    base_folder = 'cifar-100-python'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "    filename = \"cifar-100-python.tar.gz\"\n",
        "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
        "    train_list = [\n",
        "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
        "    ]\n",
        "\n",
        "    meta = {\n",
        "        'filename': 'meta',\n",
        "        'key': 'fine_label_names',\n",
        "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
        "    }\n",
        "\n",
        "    err_msg = \"Dataset поврежден или не найден. Надо указать флаг download=True\"\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, download=False):\n",
        "\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        self.train = train\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "        if download:\n",
        "          self.download()\n",
        "\n",
        "        if not self._check_data_cifar():\n",
        "          raise RuntimeError(self.err_msg)\n",
        "\n",
        "        if self.train:\n",
        "          download_list = self.train_list\n",
        "        else:\n",
        "          download_list = self.test_list\n",
        "\n",
        "        for file_name, md5_file in download_list:\n",
        "          file_path = os.path.join(self.root, self.base_folder, file_name)\n",
        "          with open(file_path, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "            self.data.append(pickle_data['data'])\n",
        "            if 'labels' in pickle_data:\n",
        "              self.targets.extend(pickle_data['labels'])\n",
        "            else:\n",
        "              self.targets.extend(pickle_data['fine_labels'])\n",
        "\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))\n",
        "        \n",
        "\n",
        "    def _load_meta(self):\n",
        "      path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
        "      if not check_integrity(path, self.meta['md5']):\n",
        "        raise RuntimeError(self.err_msg)\n",
        "      with open(path, 'rb') as f:\n",
        "        data = pickle.load(f, encoding='latin1')\n",
        "        self.classes = data[self.meta['key']]\n",
        "      self.class_to_idx = { _class: i for i, _class in enumerate(self.classes) }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       img, target = self.data[idx], self.targets[idx]\n",
        "       img = Image.fromarray(img)\n",
        "       if self.transform:\n",
        "         img = self.transform(img)\n",
        "       return img, target\n",
        "    \n",
        "    def _check_data_cifar(self):\n",
        "      for fentry in (self.train_list + self.test_list):\n",
        "          filename, md5 = fentry[0], fentry[1]\n",
        "          fpath = os.path.join(self.root, self.base_folder, filename)\n",
        "          if not check_integrity(fpath, md5):\n",
        "              return False\n",
        "      return True\n",
        "\n",
        "    def download(self):\n",
        "      if self._check_data_cifar():\n",
        "        print(\"Файлы актуальны\")\n",
        "        return\n",
        "      download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)"
      ],
      "metadata": {
        "id": "QNCtvGWr89nb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Опишем нейронную сеть"
      ],
      "metadata": {
        "id": "-636jiFFdxar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cifar100Net(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(Cifar100Net, self).__init__()\n",
        "      self.block_1 = nn.Sequential(\n",
        "           nn.Conv2d(in_channels=3, out_channels=30, kernel_size=(3), stride=(1), padding=1, bias=False),\n",
        "           nn.ReLU(),\n",
        "           nn.BatchNorm2d(30),\n",
        "           nn.Dropout2d(0.2),\n",
        "           nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "      \n",
        "      self.block_2 = nn.Sequential(\n",
        "           nn.Conv2d(in_channels=30, out_channels=60, kernel_size=(3), stride=(1), padding=1, bias=False),\n",
        "           nn.ReLU(),\n",
        "           nn.BatchNorm2d(60),\n",
        "           nn.Dropout2d(0.2),\n",
        "           nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "      self.block_3 = nn.Sequential(\n",
        "           nn.Conv2d(in_channels=60, out_channels=120, kernel_size=(3), stride=(1), padding=1, bias=False),\n",
        "           nn.ReLU(),\n",
        "           nn.BatchNorm2d(120),\n",
        "           nn.Dropout2d(0.2),\n",
        "           nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "      \n",
        "      # Планировалось в 4 блока сделать, но памяти не хватило\n",
        "      # self.block_4 = nn.Sequential(\n",
        "      #      nn.Conv2d(in_channels=120, out_channels=240, kernel_size=(3), stride=(1), padding=1, bias=False),\n",
        "      #      nn.ReLU(),\n",
        "      #      nn.BatchNorm2d(240),\n",
        "      #      nn.Dropout2d(0.2),\n",
        "      #      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "      \n",
        "      self.predict = nn.Sequential(\n",
        "          # Из-за памяти и пришлось сделать еще один слой переходящий из 1920 в 960\n",
        "          nn.Linear(1920, 960),\n",
        "          nn.Linear(960, 400),\n",
        "          nn.Linear(400, 100)\n",
        "      )\n",
        "\n",
        "  def forward(self, inp):\n",
        "    out = self.block_1(inp)\n",
        "    out = self.block_2(out)\n",
        "    out = self.block_3(out)\n",
        "    # out = self.block_4(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.predict(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "K13KZNrfd4d9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Функция для проверки"
      ],
      "metadata": {
        "id": "fjcMDuzAd9cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loader, test_loader, net, optimizer):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  best_acc = {'train': None, 'test': None}\n",
        "  net.train()\n",
        "  for epoch in range(EPOCHES):\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        \n",
        "        outputs = net(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        running_loss += loss.item()\n",
        "        running_items += len(labels)\n",
        "        \n",
        "        # выводим статистику о процессе обучения\n",
        "        if i % 200 == 0 or (i + 1) == len(train_loader):\n",
        "            net.eval()\n",
        "\n",
        "            test_loss, test_running_total, test_loss  = 0.0, 0.0, 0.0\n",
        "            for y, (out_test, lbl_test) in enumerate(test_loader):\n",
        "                test_outputs = net(out_test)\n",
        "                test_loss += loss_fn(test_outputs, lbl_test)\n",
        "                test_running_total += len(lbl_test)\n",
        "            \n",
        "            res_loss_train = running_loss / running_items\n",
        "            res_loss_test = test_loss / test_running_total\n",
        "            \n",
        "            if best_acc['train'] is None or res_loss_train < best_acc['train']:\n",
        "              best_acc['train'] = res_loss_train\n",
        "            \n",
        "            if best_acc['test'] is None or res_loss_test < best_acc['test']:\n",
        "              best_acc['test'] = res_loss_train\n",
        "\n",
        "            print(f'Epoch [{epoch + 1}/{EPOCHES}]. ' \\\n",
        "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "                  f'Loss: {res_loss_train:.3f}. '\\\n",
        "                  f'Test acc: {res_loss_test:.3f}.')\n",
        "            \n",
        "            running_loss, running_items = 0.0, 0.0\n",
        "            net.train()\n",
        "  print(f\"Best acc train: {best_acc['train']:.3f}. Best acc test: {best_acc['test']:.3f}\")\n",
        "  print('Training is finished!')"
      ],
      "metadata": {
        "id": "q85IFnaSpoww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Начнем для самописной анализ"
      ],
      "metadata": {
        "id": "4YQtEmLzpppJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Трансформация для тренеровочной выборки и для тестовой выборки\n",
        "train_transforms = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                         transforms.RandomHorizontalFlip(), \n",
        "                         transforms.ToTensor(), \n",
        "                         transforms.Normalize(*STATS, inplace=True)])\n",
        "\n",
        "valid_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                         transforms.Normalize(*STATS)])"
      ],
      "metadata": {
        "id": "tVgaEiyD6J2B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Собираем данные для обучения train=True\n",
        "train_dataset = Cifar100Dataset(root=PATH_DOWNLOAD,\n",
        "                                  train=True,\n",
        "                                  transform=train_transforms,\n",
        "                                  download=True)\n",
        "# Загружаем данные, для дальнейшего их обучения в тренеровочную сетку\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPGtwKLiEFv4",
        "outputId": "94cb94f8-3e1e-45d8-d8d0-6de834de688e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.8%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "4.7%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "7.7%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "11.1%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "16.0%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "20.9%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "23.8%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "26.9%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "29.7%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "36.1%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "42.1%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "47.8%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "55.3%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "61.8%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "68.5%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "76.1%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "83.1%IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "99.3%"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# проделываем то же самое для тестовых данных train=False\n",
        "test_dataset = Cifar100Dataset(root=PATH_DOWNLOAD, train=False,\n",
        "                                       download=True, transform=valid_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma62CtnZEqco",
        "outputId": "57926814-ca67-4865-acb6-6117a3347608"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файлы актуальны\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Cifar100Net().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "WSmafSx5dvMD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_loop(train_loader, test_loader, net, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDZxMrh6r5vK",
        "outputId": "e85690ff-f7f7-4d7c-dbe8-dea5a25b46df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]. Step [1/782]. Loss: 0.072. Test acc: 0.231.\n",
            "Epoch [1/5]. Step [257/782]. Loss: 0.314. Test acc: 0.070.\n",
            "Epoch [1/5]. Step [513/782]. Loss: 0.070. Test acc: 0.065.\n",
            "Epoch [1/5]. Step [769/782]. Loss: 0.067. Test acc: 0.061.\n",
            "Epoch [1/5]. Step [782/782]. Loss: 0.070. Test acc: 0.061.\n",
            "Epoch [2/5]. Step [1/782]. Loss: 0.066. Test acc: 0.062.\n",
            "Epoch [2/5]. Step [257/782]. Loss: 0.065. Test acc: 0.060.\n",
            "Epoch [2/5]. Step [513/782]. Loss: 0.061. Test acc: 0.055.\n",
            "Epoch [2/5]. Step [769/782]. Loss: 0.058. Test acc: 0.053.\n",
            "Epoch [2/5]. Step [782/782]. Loss: 0.060. Test acc: 0.053.\n",
            "Epoch [3/5]. Step [1/782]. Loss: 0.057. Test acc: 0.053.\n",
            "Epoch [3/5]. Step [257/782]. Loss: 0.056. Test acc: 0.052.\n",
            "Epoch [3/5]. Step [513/782]. Loss: 0.055. Test acc: 0.051.\n",
            "Epoch [3/5]. Step [769/782]. Loss: 0.055. Test acc: 0.050.\n",
            "Epoch [3/5]. Step [782/782]. Loss: 0.060. Test acc: 0.050.\n",
            "Epoch [4/5]. Step [1/782]. Loss: 0.054. Test acc: 0.050.\n",
            "Epoch [4/5]. Step [257/782]. Loss: 0.055. Test acc: 0.051.\n",
            "Epoch [4/5]. Step [513/782]. Loss: 0.055. Test acc: 0.051.\n",
            "Epoch [4/5]. Step [769/782]. Loss: 0.056. Test acc: 0.050.\n",
            "Epoch [4/5]. Step [782/782]. Loss: 0.059. Test acc: 0.051.\n",
            "Epoch [5/5]. Step [1/782]. Loss: 0.052. Test acc: 0.052.\n",
            "Epoch [5/5]. Step [257/782]. Loss: 0.056. Test acc: 0.050.\n",
            "Epoch [5/5]. Step [513/782]. Loss: 0.055. Test acc: 0.050.\n",
            "Epoch [5/5]. Step [769/782]. Loss: 0.056. Test acc: 0.051.\n",
            "Epoch [5/5]. Step [782/782]. Loss: 0.059. Test acc: 0.050.\n",
            "Best acc train: 0.052. Best acc test: 0.059\n",
            "Training is finished!\n",
            "CPU times: total: 56min 29s\n",
            "Wall time: 9min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение классификатора картинок на примере CIFAR-100 (датасет можно изменить) через дообучение ImageNet Resnet-50  с аугментацией данных."
      ],
      "metadata": {
        "id": "A2hXv8x4Az4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet50(pretrained=True)"
      ],
      "metadata": {
        "id": "hNkoxKhXBSul"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "NJHl80TcBrL9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.fc = nn.Linear(2048, 100)"
      ],
      "metadata": {
        "id": "D1T-ypuyBsS_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Трансформация для тренеровочной выборки и для тестовой выборки\n",
        "train_transforms = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                         transforms.RandomHorizontalFlip(), \n",
        "                         transforms.ToTensor(), \n",
        "                         transforms.Normalize(*STATS, inplace=True)])\n",
        "\n",
        "valid_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                         transforms.Normalize(*STATS)])"
      ],
      "metadata": {
        "id": "HdO4OViFB-9t"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Собираем данные для обучения train=True\n",
        "train_dataset = Cifar100Dataset(root=PATH_DOWNLOAD,\n",
        "                                  train=True,\n",
        "                                  transform=train_transforms,\n",
        "                                  download=True)\n",
        "# Загружаем данные, для дальнейшего их обучения в тренеровочную сетку\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_ywpALaCBiG",
        "outputId": "bdffdf5e-ff7a-4151-e60f-094c0cdd37b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файлы актуальны\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name,param in net.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=0.001)"
      ],
      "metadata": {
        "id": "I4YwcjcXCDqF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_loop(train_loader, test_loader, net, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxyJ8njKCp_d",
        "outputId": "5981a64d-c73d-4707-b663-adade4bf211c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5]. Step [1/782]. Loss: 0.074. Test acc: 0.079.\n",
            "Epoch [1/5]. Step [257/782]. Loss: 0.061. Test acc: 0.055.\n",
            "Epoch [1/5]. Step [513/782]. Loss: 0.055. Test acc: 0.054.\n",
            "Epoch [1/5]. Step [769/782]. Loss: 0.053. Test acc: 0.052.\n",
            "Epoch [1/5]. Step [782/782]. Loss: 0.057. Test acc: 0.053.\n",
            "Epoch [2/5]. Step [1/782]. Loss: 0.051. Test acc: 0.053.\n",
            "Epoch [2/5]. Step [257/782]. Loss: 0.051. Test acc: 0.051.\n",
            "Epoch [2/5]. Step [513/782]. Loss: 0.052. Test acc: 0.051.\n",
            "Epoch [2/5]. Step [769/782]. Loss: 0.051. Test acc: 0.050.\n",
            "Epoch [2/5]. Step [782/782]. Loss: 0.057. Test acc: 0.051.\n",
            "Epoch [3/5]. Step [1/782]. Loss: 0.056. Test acc: 0.050.\n",
            "Epoch [3/5]. Step [257/782]. Loss: 0.050. Test acc: 0.052.\n",
            "Epoch [3/5]. Step [513/782]. Loss: 0.050. Test acc: 0.050.\n",
            "Epoch [3/5]. Step [769/782]. Loss: 0.050. Test acc: 0.051.\n",
            "Epoch [3/5]. Step [782/782]. Loss: 0.054. Test acc: 0.049.\n",
            "Epoch [4/5]. Step [1/782]. Loss: 0.041. Test acc: 0.049.\n",
            "Epoch [4/5]. Step [257/782]. Loss: 0.049. Test acc: 0.050.\n",
            "Epoch [4/5]. Step [513/782]. Loss: 0.049. Test acc: 0.049.\n",
            "Epoch [4/5]. Step [769/782]. Loss: 0.049. Test acc: 0.048.\n",
            "Epoch [4/5]. Step [782/782]. Loss: 0.050. Test acc: 0.049.\n",
            "Epoch [5/5]. Step [1/782]. Loss: 0.054. Test acc: 0.049.\n",
            "Epoch [5/5]. Step [257/782]. Loss: 0.048. Test acc: 0.049.\n",
            "Epoch [5/5]. Step [513/782]. Loss: 0.049. Test acc: 0.050.\n",
            "Epoch [5/5]. Step [769/782]. Loss: 0.049. Test acc: 0.049.\n",
            "Epoch [5/5]. Step [782/782]. Loss: 0.050. Test acc: 0.050.\n",
            "Best acc train: 0.041. Best acc test: 0.041\n",
            "Training is finished!\n",
            "CPU times: total: 2h 3min 41s\n",
            "Wall time: 20min 51s\n"
          ]
        }
      ]
    }
  ]
}